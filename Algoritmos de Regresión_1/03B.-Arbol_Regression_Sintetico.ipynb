{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# √Årbol de Decisi√≥n de Regresi√≥n ‚Äî Dataset Sint√©tico\n",
        "\n",
        "Este cuaderno genera un dataset sint√©tico con **no linealidades**, **outliers** y **faltantes**, para entrenar y evaluar un √°rbol de decisi√≥n de regresi√≥n.\n",
        "\n",
        "Incluye:\n",
        "- Generaci√≥n y **limpieza** (capado IQR + imputaci√≥n de medianas)\n",
        "- Divisi√≥n train/test\n",
        "- Modelo base y **tuning** con `GridSearchCV`\n",
        "- **Curva de aprendizaje**, **importancias** y **visualizaci√≥n del √°rbol** (truncado)\n",
        "\n",
        "> Nota: Se usa `matplotlib` sin estilos predefinidos y una figura por gr√°fico.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import math\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def cap_outliers_iqr(df, cols):\n",
        "    capped = df.copy()\n",
        "    for c in cols:\n",
        "        q1 = capped[c].quantile(0.25)\n",
        "        q3 = capped[c].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower = q1 - 1.5 * iqr\n",
        "        upper = q3 + 1.5 * iqr\n",
        "        capped[c] = np.clip(capped[c], lower, upper)\n",
        "    return capped\n",
        "\n",
        "def report_metrics(y_true, y_pred, label=\"\"):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = math.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"{label}MAE:  {mae:.4f}\")\n",
        "    print(f\"{label}MSE:  {mse:.4f}\")\n",
        "    print(f\"{label}RMSE: {rmse:.4f}\")\n",
        "    print(f\"{label}R¬≤:   {r2:.4f}\")\n",
        "\n",
        "def plot_learning_curve(estimator, X, y, title=\"Curva de aprendizaje\"):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=None,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 5), shuffle=True, random_state=42\n",
        "    )\n",
        "    train_rmse = np.sqrt(-train_scores)\n",
        "    test_rmse = np.sqrt(-test_scores)\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Tama√±o de entrenamiento\")\n",
        "    plt.ylabel(\"RMSE (CV)\")\n",
        "    plt.plot(train_sizes, train_rmse.mean(axis=1), marker='o', label=\"Entrenamiento\")\n",
        "    plt.plot(train_sizes, test_rmse.mean(axis=1), marker='s', label=\"Validaci√≥n CV\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generaci√≥n y limpieza de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "X_syn, y_syn = make_regression(n_samples=1200, n_features=8, n_informative=6, noise=10.0, random_state=42)\n",
        "X_syn = pd.DataFrame(X_syn, columns=[f\"x{i}\" for i in range(8)])\n",
        "y_syn = pd.Series(y_syn, name=\"target\")\n",
        "\n",
        "# A√±adir no linealidades e interacciones\n",
        "X_syn[\"x_sin\"] = np.sin(X_syn[\"x0\"]) * 10\n",
        "X_syn[\"x_interact\"] = X_syn[\"x1\"] * X_syn[\"x2\"]\n",
        "\n",
        "# Outliers en ~1.5%\n",
        "n_out = int(0.015 * len(X_syn))\n",
        "rows_out = np.random.choice(X_syn.index, size=n_out, replace=False)\n",
        "X_syn.loc[rows_out, \"x3\"] *= 10\n",
        "y_syn.loc[rows_out] *= 5\n",
        "\n",
        "# Faltantes en ~2% de algunas columnas\n",
        "for c in [\"x0\", \"x4\", \"x_interact\"]:\n",
        "    mask = np.random.rand(len(X_syn)) < 0.02\n",
        "    X_syn.loc[mask, c] = np.nan\n",
        "\n",
        "print(\"Forma de X_synthetic:\", X_syn.shape)\n",
        "print(\"Faltantes por columna:\\n\", X_syn.isna().sum())\n",
        "\n",
        "# Capado IQR (usamos una imputaci√≥n temporal de medianas para poder capar sin NaNs)\n",
        "X_syn_capped = cap_outliers_iqr(X_syn.fillna(X_syn.median(numeric_only=True)), X_syn.columns)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X_syn_capped, y_syn, test_size=0.2, random_state=42)\n",
        "\n",
        "num_features_syn = list(Xtr.columns)\n",
        "preprocess_syn = ColumnTransformer(\n",
        "    transformers=[(\"num\", SimpleImputer(strategy=\"median\"), num_features_syn)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelo base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe_syn = Pipeline([\n",
        "    (\"prep\", preprocess_syn),\n",
        "    (\"model\", DecisionTreeRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "pipe_syn.fit(Xtr, ytr)\n",
        "pred_tr_syn = pipe_syn.predict(Xtr)\n",
        "pred_te_syn = pipe_syn.predict(Xte)\n",
        "\n",
        "print(\"Rendimiento base:\")\n",
        "report_metrics(ytr, pred_tr_syn, label=\"Train \")\n",
        "report_metrics(yte, pred_te_syn, label=\"Test  \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Curva de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plot_learning_curve(DecisionTreeRegressor(random_state=42), Xtr, ytr, title=\"Curva de aprendizaje ‚Äî √Årbol base (Sint√©tico)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning con GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "param_grid_syn = {\n",
        "    \"model__max_depth\": [None, 4, 6, 10],\n",
        "    \"model__min_samples_split\": [2, 5, 10],\n",
        "    \"model__min_samples_leaf\": [1, 2, 5],\n",
        "    \"model__ccp_alpha\": [0.0, 0.0005, 0.001, 0.01]\n",
        "}\n",
        "grid_syn = GridSearchCV(pipe_syn, param_grid_syn, cv=5, scoring=\"neg_mean_squared_error\")\n",
        "grid_syn.fit(Xtr, ytr)\n",
        "print(\"Mejores par√°metros (sint√©tico):\", grid_syn.best_params_)\n",
        "\n",
        "best_syn = grid_syn.best_estimator_\n",
        "pred_tr_best = best_syn.predict(Xtr)\n",
        "pred_te_best = best_syn.predict(Xte)\n",
        "print(\"\\nRendimiento con tuning:\")\n",
        "report_metrics(ytr, pred_tr_best, label=\"Train \")\n",
        "report_metrics(yte, pred_te_best, label=\"Test  \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importancias y √°rbol truncado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "final_tree_syn = best_syn.named_steps[\"model\"]\n",
        "importances_syn = final_tree_syn.feature_importances_\n",
        "plt.figure()\n",
        "order_syn = np.argsort(importances_syn)[::-1]\n",
        "plt.bar(range(len(importances_syn)), importances_syn[order_syn])\n",
        "plt.xticks(range(len(importances_syn)), np.array(num_features_syn)[order_syn], rotation=45, ha='right')\n",
        "plt.title(\"Importancia de caracter√≠sticas ‚Äî Sint√©tico\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_tree(final_tree_syn, max_depth=3, filled=True, feature_names=num_features_syn)\n",
        "plt.title(\"√Årbol de decisi√≥n (truncado) ‚Äî Sint√©tico\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones\n",
        "- El √°rbol maneja bien relaciones no lineales y outliers con el flujo de limpieza.\n",
        "- El tuning mejora la generalizaci√≥n.\n",
        "- Puedes probar ensambles (RandomForest, Gradient Boosting) para mayor robustez.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Interpretaci√≥n autom√°tica del modelo\n",
        "Esta celda resume el desempe√±o del modelo con un texto interpretativo y detecta posibles signos de sobreajuste o subajuste.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import math\n",
        "\n",
        "# Predicciones del mejor modelo\n",
        "y_pred_tr_best = best_syn.predict(Xtr)\n",
        "y_pred_te_best = best_syn.predict(Xte)\n",
        "\n",
        "# M√©tricas\n",
        "mae_tr = mean_absolute_error(ytr, y_pred_tr_best)\n",
        "mse_tr = mean_squared_error(ytr, y_pred_tr_best)\n",
        "rmse_tr = math.sqrt(mse_tr)\n",
        "r2_tr = r2_score(ytr, y_pred_tr_best)\n",
        "\n",
        "mae_te = mean_absolute_error(yte, y_pred_te_best)\n",
        "mse_te = mean_squared_error(yte, y_pred_te_best)\n",
        "rmse_te = math.sqrt(mse_te)\n",
        "r2_te = r2_score(yte, y_pred_te_best)\n",
        "\n",
        "print(\"Resumen de m√©tricas (mejor modelo):\")\n",
        "print(f\"Train -> MAE: {mae_tr:.3f} | RMSE: {rmse_tr:.3f} | R¬≤: {r2_tr:.4f}\")\n",
        "print(f\"Test  -> MAE: {mae_te:.3f} | RMSE: {rmse_te:.3f} | R¬≤: {r2_te:.4f}\\n\")\n",
        "\n",
        "# Interpretaci√≥n en texto\n",
        "def nivel(valor, low, mid, high):\n",
        "    if valor < low: return \"bajo\"\n",
        "    if valor < mid: return \"medio\"\n",
        "    if valor < high: return \"bueno\"\n",
        "    return \"muy bueno\"\n",
        "\n",
        "gap = r2_tr - r2_te\n",
        "nivel_test = nivel(r2_te, 0.2, 0.5, 0.7)\n",
        "\n",
        "print(\"üß† Interpretaci√≥n:\")\n",
        "print(f\"- El modelo explica aproximadamente el {r2_te*100:.1f}% de la variabilidad en el conjunto de prueba (R¬≤ {nivel_test}).\")\n",
        "print(f\"- El error promedio absoluto (MAE) en prueba es {mae_te:.2f}, y el RMSE es {rmse_te:.2f}.\")\n",
        "if gap > 0.15:\n",
        "    print(f\"- Existe indicio de **sobreajuste** (diferencia Train-Test de R¬≤ = {gap:.2f}). Ajusta hiperpar√°metros (max_depth, min_samples_leaf/split, ccp_alpha) o usa ensambles.\")\n",
        "elif gap < -0.05:\n",
        "    print(f\"- El desempe√±o en prueba es **mejor** que en entrenamiento (gap R¬≤ = {gap:.2f}); revisa la aleatoriedad o la estabilidad con validaci√≥n cruzada.\")\n",
        "else:\n",
        "    print(f\"- La generalizaci√≥n es **razonable** (gap R¬≤ = {gap:.2f}).\")\n",
        "print(\"- Prueba tambi√©n RandomForestRegressor o GradientBoostingRegressor para mayor robustez.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Comparativa de m√©tricas ‚Äî Antes vs. Despu√©s del *tuning*\n",
        "La siguiente tabla resume **MAE**, **RMSE** y **R¬≤** para el modelo base y el mejor modelo tras *GridSearchCV*, en **train** y **test**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 'pipe_syn' entrenado antes del tuning; 'best_syn' es el mejor tras GridSearchCV.\n",
        "y_tr_base = pipe_syn.predict(Xtr)\n",
        "y_te_base = pipe_syn.predict(Xte)\n",
        "\n",
        "y_tr_best = best_syn.predict(Xtr)\n",
        "y_te_best = best_syn.predict(Xte)\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = math.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mae, rmse, r2\n",
        "\n",
        "rows = []\n",
        "rows.append([\"Base\", \"Train\", *metrics(ytr, y_tr_base)])\n",
        "rows.append([\"Base\", \"Test\",  *metrics(yte, y_te_base)])\n",
        "rows.append([\"Tuned\", \"Train\", *metrics(ytr, y_tr_best)])\n",
        "rows.append([\"Tuned\", \"Test\",  *metrics(yte, y_te_best)])\n",
        "\n",
        "df_comp = pd.DataFrame(rows, columns=[\"Modelo\",\"Split\",\"MAE\",\"RMSE\",\"R2\"])\n",
        "display(df_comp.round(4))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}